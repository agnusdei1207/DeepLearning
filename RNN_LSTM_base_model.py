# -*- coding: utf-8 -*-
"""RNN, LSTM Base Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ReP0Ln-knOc3_FLeDxbCt1THcbopU_ZC

### RNN base model / text mining of News
"""

# 로이터 뉴스 로딩
from tensorflow.keras.datasets import reuters
import numpy as np
import matplotlib.pyplot as plt

# num_words : 해당 랭킹까지만 데이터로 인정
# oov_char : 인정받지 않은 값들을 파라미터로 채운다.
(X_train, y_train),(X_test, y_test) = reuters.load_data(num_words=2000, oov_char=2)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

print(X_train[0]) # 첫번째 뉴스기사 정보 : 단어 중심 토큰화 및 등장 빈도수를 기준 랭킹 순번
# 모든 뉴스 기사는 시작이 1로 맵핑 : 1 >> 뉴스 기사가 시작됩니다 라는 의미

reuters.get_word_index() # 랭킹

np.unique(y_train) # 정답 데이터 유니크 값 46가지 : 카테고리 예측

"""### RNN 을 활용하여 46가지 카테로리로 기사를 분류하자!"""

# 학습을 위해 데이터 모양을 맞추자 : (sample, time step, feature) 

# 데이터 길이가 각기 다르다
print(len(X_train[0])) # 87
print(len(X_train[100])) # 409

# 단어의 개수 >> time step 
# time step 을 동일하게 만들어 주어야 한다.

# 시퀀스 길이 맞추기
# 훈련 데이터에 있는 뉴스 기사들을 하나씩 다 꺼내서 기사 길이를 측청한 후 리스트에 담았습니다.
X_train_len = [len(doc) for doc in X_train]

# 중앙 값 및 평균 확인하기
print("최소 :", min(X_train_len))
print("최대 :", max(X_train_len))
print("중앙 :", np.median(X_train_len))
print("평균 :", np.mean(X_train_len)) # 이상치에 영향을 크게 받는다.

plt.hist(X_train_len, bins=20)
plt.show()

# 훈련과 테스트 데이터의 기사 길이가 전부 다르기에 길이를 똑같이 맞춰주자!
from tensorflow.keras.preprocessing import sequence

X_train_pad = sequence.pad_sequences(X_train, maxlen = 120) # 최대길이 120
X_test_pad = sequence.pad_sequences(X_test, maxlen = 120) # 길이가 작다면 pad 효과로 빈 공백을 0 으로 채운다.

# 자른 길이 확인
print(X_train_pad.shape) # 8982, 120 , no feature
print(X_test_pad.shape) # 2246, 120, no feature

# feature 값 채우기 : 단어의 빈도만을 가지고 평가하기에 1 을 넣는다.
X_train_pad_reshape = X_train_pad.reshape(8982, 120, 1)
X_test_pad_reshape = X_test_pad.reshape(2246, 120, 1)

"""### Simple RNN

- CNN 구조 : (sample, width, height, color)
- RNN 구조 : (sample, time step, feature) 
  1. time step : 
    ex) daddy 의 경우 time step == 4, 도레미 ~ ? time step == 3 / 앞의 기억 데이터
  2. feature : 학습시킬 때 세트 내부의 데이터 종류 개수
    ex) 음정과 박자를 입력하여 다음 음을 예측해라! 음+박자 두 개의 데이터이므로 
    featrue == 2
"""

# RNN : Deep learning 기본 모델 + 기억값
# RNN Simple model 단점 : 기울기 소실 >> 가까운 기억값이 크게 작용하고 멀리 있는 기억값은 작은 영향력을 갖는다.
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer, Dense, SimpleRNN

model = Sequential()
# 단어를 순차적으로 넣어서 120번 순환한다.
model.add(InputLayer(input_shape=(120, 1))) # Input_size : 데이터 하나가 가진 shape 
model.add(SimpleRNN(128)) # 넣고 싶은 숫자~
model.add(Dense(64, activation="relu")) # 항아리 모양을 위한 임시적 층
model.add(Dense(46, activation="softmax")) # 분류하고자 하는 종류의 개수

model.compile(loss = "sparse_categorical_crossentropy", # 정답 데이터를 확률정보로 바꾸기 위해 sparse_categorical_crossentropy 사용
              optimizer = "Adam",
              metrics = ["Accuracy"])

h = model.fit(X_train_pad_reshape, y_train, validation_split=0.2, epochs=20)

plt.figure(figsize=(15,5))
plt.plot(h.history["Accuracy"], label="accuracy")
plt.plot(h.history["val_Accuracy"], label="val_accuracy")
plt.show()

"""### LSTM (Long Short Term Memory)
  - 기억에 관한 관리 기능이 추가된 모델
  - 멀리있거나 가까이 있는 기억의 중요도를 측정하고 가중치를 부여하여 더욱 오래가도록 유지시킨다.
  - 반대로 중요도가 없다고 판단되면 빠르게 삭제시킨다.
  - 과거의 기억 값이 흐려지지 않도록, 불필요한 기억은 빠르게 잊도록 하는 알고리즘
"""

from tensorflow.keras.layers import LSTM
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer, Dense, SimpleRNN

model = Sequential()
# 단어를 순차적으로 넣어서 120번 순환한다.
model.add(InputLayer(input_shape=(120, 1))) # Input_size : 데이터 하나가 가진 shape 
model.add(LSTM(128)) # LSTM / 넣고 싶은 숫자~
model.add(Dense(64, activation="relu")) # 항아리 모양을 위한 임시적 층
model.add(Dense(46, activation="softmax")) # 분류하고자 하는 종류의 개수

model.compile(loss = "sparse_categorical_crossentropy", # 정답 데이터를 확률정보로 바꾸기 위해 sparse_categorical_crossentropy 사용
              optimizer = "Adam",
              metrics = ["Accuracy"])

h = model.fit(X_train_pad_reshape, y_train, validation_split=0.2, epochs=1000)

# 워드 임베딩 : 단어 의미의 본질을 이해하기 위해 의미에 따라 표기된 실수가 필요
# 단어의 의미가 가까울수록 실제 숫자 값도 가깝게 맵핑!
from tensorflow.keras.layers import Embedding

# 모델 구조 설계
# Embedding 가장 앞쪽에 배치해야 한다.
model3 = Sequential()
model3.add(Embedding(1000, 50)) # 사용하는 단어 수, 각 단어를 표현할 숫자 수 (한 단어를 50개로 나누며 50개의 관점으로 단어를 바라 봄)
model3.add(LSTM(128, return_sequences=True)) # default 구조 : 다수입력 단일출력 / 층을 쌓기 위해 다수입력 다수출력 구조로 바꾸기~
model3.add(LSTM(128, return_sequences=True)) # 층을 추가 할 수 있다.
model3.add(LSTM(128))
model3.add(Dense(32, activation="relu"))
model3.add(Dense(46, activation="softmax"))

model3.compile(loss="sparse_categorical_crossentropy",
               loss="Adam",
               metrics=["accuracy"])

