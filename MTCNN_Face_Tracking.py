# -*- coding: utf-8 -*-
"""face_tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DGhuDdFJtqGLPDb9oBQ-RaS-sHpBdlfA
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Face tracking pipeline

The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and tracking on an image dataset using MTCNN.
"""

!pip3 install facenet
!pip3 install MTCNN
!pip3 install facenet_pytorch
!pip3 install opencv-python
!pip3 install MMCV
!pip3 install IPython
!pip3 install Ipython display

from facenet_pytorch import MTCNN # MTCNN 모델 가져오기
import torch
import numpy as np
import mmcv, cv2
from PIL import Image, ImageDraw
from IPython.display  import display
from torchvision import datasets, transforms
#HTML 확인용
from IPython.display import HTML
from base64 import b64encode

"""#### Determine if an nvidia GPU is available"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # cuda GPU 를 사용하며, 없을 시 cpu 사용
print('Running on device: {}'.format(device))

"""#### Define MTCNN module

Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.

See `help(MTCNN)` for more details.
"""

mtcnn = MTCNN(keep_all=True, device=device) # mtcnn 모델 가져오기

"""#### Get a sample video

We begin by loading a video with some faces in it. The `mmcv` PyPI package by mmlabs is used to read the video frames (it can be installed with `pip install mmcv`). Frames are then converted to PIL images.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/project2/FaceDetect4/facenet-pytorch-master/examples"
!pwd

video = mmcv.VideoReader("./video.mp4") # mmcv 비디오 읽어오기
frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video] # 프레임별로 cvt 시킨 후 저장

mp4 = open('./video.mp4','rb').read()
data_url= "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)
# display.Video('video.mp4', width=640)

"""#### Run video through MTCNN

We iterate through each frame, detect faces, and draw their bounding boxes on the video frames.
"""

frames_tracked = [] # 얼굴 추적 프레임을 저장할 리스트

# enumerate : 반복문 사용 시 몇 번째 반복문인지 확인할 떄 사용, 인덱스 번호와 원소를 tuple형태로 반환합니다.
for i, frame in enumerate(frames): # frame : 튜플형태로 사진 데이터 저장 / i : 인덱스 번호
    print('\r프레임: {}'.format(i + 1)) # 각각의 프레임 출력
    
    # 얼굴 탐지
    boxes, _ = mtcnn.detect(frame) # mtcnn 모델을 활용하여 한 프레임씩 탐지 후 박스에 결과 저장 
    # boxes : 2차원의 배열 (21, 4) # 행 : 감지된 사람의 얼굴 수 / 열 : 해당 정보의 차원 및 좌표
    # _ :  프레임 한 장에 탐지된 사람들의 얼굴 확률 값
    
    # 얼굴 그리기
    frame_draw = frame.copy() # 프레임 한 장 카피
    draw = ImageDraw.Draw(frame_draw) 
        
    # 감지된 사람 수가 0이라면 모델에 오류가 발생함 이를 해결하기 위한 알고리즘
    # 그림 그려주기
    try:
        for box in boxes:
            if box is not None :
                # 네모 그리기
                draw.rectangle(box.tolist(), outline=(255, 0, 0), width=1) # tolist : list 형태로 대상의 차원에 맞춰서 반환, 얼굴 탐지가 된 좌표값
                #outline : 네모 색상 / width : 두께
                frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR)) # 리스트에 추가
                # BILINEAR : 이미지 리사이즈 시 이진선형을 사용하여 리사이즈 하겠다.
    except:
         print("except!!") # 가독성을 위한 띄어쓰기

print('탐색 끝!')

"""#### Display detections"""

frames_tracked[100]

d = display(frames_tracked[0], display_id=True) # display_id : str, bool 선택 사항
#     디스플레이의 ID를 설정합니다.
#     이 ID는 나중에 update_display를 통해 이 표시 영역을 업데이트하는 데 사용할 수 있습니다.
#     'True'로 지정된 경우 새 'display_id'를 생성합니다.
#      kwargs: 추가 키워드 인수, 선택 사항
#     추가 키워드 인수는 디스플레이 게시자에게 전달됩니다.

i = 1
try:
    while True:
        if i < len(frames_tracked):    
            d.update(frames_tracked[i]) # 사진 업데이트
            i += 1
        else:
            break
except:
    print("except!!")

"""#### Save tracked video"""

dim = frames_tracked[0].size # (640, 360)
fourcc = cv2.VideoWriter_fourcc(*'FMP4')  
video_tracked = cv2.VideoWriter("./저장.mp4", fourcc, 25.0, dim) 
# outputFile (str) – 저장할 파일 & 경로
# fourcc – Codec정보 >>  cv2.VideoWriter_fourcc()
# frame (float) – 초당 저장될 frame
# size (list) – 저장될 사이즈(ex; 640, 480)
for frame in frames_tracked:
    video_tracked.write(cv2.cvtColor(np.array(frame), 4)) # np 배열, cv2.COLOR_RGB2BGR = 4 색변환
video_tracked.release() # 열려있는 비디오 닫기 close == release
