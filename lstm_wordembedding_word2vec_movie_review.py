# -*- coding: utf-8 -*-
"""LSTM_WordEmbedding_Word2Vec_Movie_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FG7vhbHaD6ANuPauyDWE3qgxN5OvCnaE
"""

import pandas as pd

train = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/ratings_train.txt", delimiter="\t") # 구분문자 tab
test = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/ratings_test.txt", delimiter="\t")

train.head()

train.dropna(inplace=True) # 결측치 삭제
test.dropna(inplace=True) # 결측치 삭제 

print(train.shape)
print(test.shape)

X_train = train["document"]
y_train = train["label"]
X_test = test["document"]
y_test = test["label"]

# 한글 형태소 분석기 다운로드
!pip install konlpy

from konlpy.tag import Okt # kkma 에 비해선 가벼운 형태소 분류
from tqdm import tqdm # 반복작업 시 얼마나 진행이 되는지 확인

okt = Okt()
okt.tagset

X_train_morphs = []
for doc in tqdm(X_train):
  morphs = okt.morphs(doc)
  X_train_morphs.append(" ".join(morphs))

X_test_morphs = []
for doc in tqdm(X_test):
  morphs = okt.morphs(doc)
  X_test_morphs.append(" ".join(morphs))

import pickle # 피클을 이용해서 파일로 저장
# with : python 의 파일을 여는 명령어이며 자동으로 close 가 된다.
with open("/content/drive/MyDrive/Colab Notebooks/data/X_train_morphs.pkl", "wb") as f: # wb == write binary 쓰기 모드 
  pickle.dump(X_train_morphs, f)

with open("/content/drive/MyDrive/Colab Notebooks/data/X_test_morphs.pkl", "wb") as f:
  pickle.dump(X_test_morphs, f)

X_train_morphs[:10]

"""## 토큰화 및 수치화 작업"""

from tensorflow.keras.preprocessing.text import Tokenizer # 띄어쓰기를 기준으로 토큰화 및 빈도수 랭킹화 모듈


# 데이터 탐색
tokenizer = Tokenizer(num_words = 5000) # 랭킹 5000위 이내만 사용
tokenizer.fit_on_texts(X_train_morphs)

len(tokenizer.word_index) # 102054 개의 유니크 값 (토큰)
tokenizer.word_index # 빈도 랭킹

tokenizer.word_counts # 토큰 및 빈도

wc = pd.DataFrame(tokenizer.word_counts.items())  # 데이터 프레임으로 만들면, 단어는 0번째 빈도수는 1 번째 column으로 들어간다.
wc

sorted_wc = wc.sort_values(by=1, ascending=False) # 1번열을 기준으로 내림차순 정렬
sorted_wc.reset_index()[[0,1]] # 0, 1 컬럼 출력

sorted_wc[sorted_wc[1] >= 10] # 빈도가 10번 이상인 토큰
sorted_wc[sorted_wc[1] < 2] # 빈도가 한 번 뿐인 토큰

# num_words 결정 및 저장
# 단어의 빈도수를 몇 위까지 설정하면 좋을까? num_words = ?

real_tokenizer = Tokenizer(num_words = 78000)
real_tokenizer.fit_on_texts(X_train_morphs)

# 댓글 정보를 랭킹 정보로 변환
X_train_seq = real_tokenizer.texts_to_sequences(X_train_morphs)
X_test_seq = real_tokenizer.texts_to_sequences(X_test_morphs)

print(X_train_seq[10010]) # 인덱스 마다 길이가 전부 다르다.
print(X_train_seq[0])

"""## RNN 학습을 위한 데이터 전처리"""

# 길이만을 모아둔 리스트를 만들어서 길이들의 최소, 최대, 중앙, 평균값을 탐색하자
# 길이가 다양하다
X_train_len = [len(doc) for doc in X_train_seq]
X_train_len

import numpy as np

print(max(X_train_len))
print(min(X_train_len))
print(np.median(X_train_len))
print(np.mean(X_train_len))

# 길이가 0 인 리뷰 제거 / 빈도수가 낮은 단어를 제거했기에 추가적인 결측치 발생!
X_train_seq_rm = []
y_train_rm = []
# zip : 리스트를 집어 넣으면 같은 인덱스를 가진 아이템끼리 하나로 묶어준다.
for x, y in zip(X_train_seq, y_train): # X_train[0]번째를 꺼냈으면 y_train 도 [0]번째를 꺼낸다.
  if len(x) != 0:
    X_train_seq_rm.append(x)
    y_train_rm.append(y)

print(len(X_train_seq_rm), len(y_train_rm))

# 테스트 데이터에도 0인 리뷰가 있을 수 있으니 전처리!
X_test_seq_rm = []
y_test_rm = []
for x, y in zip(X_test_seq_rm, y_test):
  if len(x) != 0:
    X_test_seq_rm.append(x)
    y_test_rm.append(y)

# 표를 통해 적절한 길이를 탐색하자!

import matplotlib.pyplot as plt
# histogram : 분포도
plt.hist(X_train_len, bins=20) # 히스토그램 막대 개수
plt.show()

# 시퀀스 길이를 맞추자
from tensorflow.keras.preprocessing import sequence

X_train_pad = sequence.pad_sequences(X_train_seq_rm, maxlen = 20) # pad_seqences : maxlen을 초과하는 빈 공간은 0으로 채운다
X_test_pad = sequence.pad_sequences(X_test_seq_rm, maxlen = 20)

# 길이 일치
print(len(X_train_pad))
print(len(y_train_rm))

X_train_pad[0].shape

"""# 모델링
  - Word Embedding (사용하는 단어의 총 개수, 각각의 단어들을 표현할 숫자의 개수)
  - LSTM 레이어 활용
  - 출력층 이진 분류
  - Compile 할 때 Binary Crossentropy
  - history 활용해서 그래프를 그릴 수 있게
  - Model check point
  - Early Stopping
"""

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd /content/drive/MyDrive/Colab Notebooks/

from tensorflow.keras.layers import LSTM
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer, Dense, SimpleRNN 
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import Embedding


model = Sequential()
# 임베딩 작업이 필요하다면 가장 처음에 임베딩 작업을 한다.
model.add(Embedding(149609, 100)) # 사용하는 단어 총 개수, 각각의 단어들를 표현할 숫자 수 (한 단어를 50개로 나누며 50개의 관점으로 단어를 바라 봄)
model.add(LSTM(128, return_sequences=True)) # LSTM / 넣고 싶은 숫자~
model.add(LSTM(256, return_sequences=True)) # LSTM / 넣고 싶은 숫자~
model.add(LSTM(128)) # LSTM / 넣고 싶은 숫자~
model.add(Dense(64, activation="sigmoid")) # 항아리 모양을 위한 임시적 층
model.add(Dense(1, activation="sigmoid")) # 분류하고자 하는 종류의 개수 / 이진분류 1

model.compile(loss = "binary_crossentropy", # 이진분류 
              optimizer = "Adam",
              metrics = ["accuracy"]) # 여기 기준 대소문자 맞추기

# 체크포인트 저장
# 모델이 저장될 경로를 문자열로 저장
save_path = "./data/model/model1.{epoch:03d}_{val_accuracy:.4f}.hdf5"  # d : 10진수 정수형태  / 03 : 3자리 수로 표현 / .4f : 소수점 4째 자리까지 표현 / {} 대소문자 확인
mckp = ModelCheckpoint(filepath = save_path,
                                                  monitor = "val_accuracy",
                                                    save_best_only = True, # 최고값을 갱신했을 때만 저장해라
                                                        verbose = 1)

# 조기 학습 중단
early = EarlyStopping(monitor = "val_accuracy",
                                          patience = 10)  # patience : ?번 만큼 봐주는 데 개선이 안 되면 중단!

h = model.fit(X_train_pad,
              np.array(y_train_rm),
              validation_split=0.2,
              epochs=1000,
              callbacks=[mckp, early],
              batch_size = 64) # batch_size : 한번에 몇개의 데이터를 넣을래?

plt.figure(figsize=(15,5))
plt.plot(h.history["accuracy"], label="accuracy") # 대소문자 구분
plt.plot(h.history["val_accuracy"], label="val_accuracy")
plt.show()

"""### 모델 평가"""

from tensorflow.keras.models import load_model

m = load_model("./data/model/model1.001_0.8552.hdf5")

m.evaluate(X_train_pad, np.array(y_train_rm))

"""## Word2Vec 사용하기"""

!pip install gensim # word2vec 을 지원하는 라이브러리

from gensim.models import Word2Vec

with open("/content/drive/MyDrive/Colab Notebooks/data/X_test_morphs.pkl", "rb") as f: # 읽기모드
  data2 = pickle.load(f)

with open("/content/drive/MyDrive/Colab Notebooks/data/X_train_morphs.pkl", "rb") as f: # 읽기모드
  data = pickle.load(f)

# 띄어쓰기 단위로 문장이 쪼개지게 만들기
data_split = []
for d in data:
  data_split.append(d.split(" "))

word2vec = Word2Vec(data, # 임베딩 학습시킬 데이터
                    size=100, # 단어 한 개당 표현할 숫자의 개수
                    min_count=5, # 단어의 최소 등장 개수
                    sg = 1, # skip gram : 중심단어 x, 주변단어 y # CBOW : 주변단어 x, 중심단어 y
                    window=5 # 주변 단어 개수
                    )

len(word2vec.wv.vectors)

word2vec.wv.most_similar("a") # 단어장에 저장 된 단어를 사용

