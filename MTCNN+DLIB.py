# -*- coding: utf-8 -*-
"""MTCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DGhuDdFJtqGLPDb9oBQ-RaS-sHpBdlfA
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/project2/FaceDetect4/facenet-pytorch-master/examples/"
!pwd

# MTCNN 환경 셋팅
!pip3 install facenet
!pip3 install MTCNN
!pip3 install facenet_pytorch
!pip3 install opencv-python
!pip3 install MMCV
!pip3 install IPython
!pip3 install Ipython display
# Dlib 환경 셋팅
!pip install dlib # 68 of Landmark from face
!pip install imutils
!pip install cmake

# MTCNN 
from facenet_pytorch import MTCNN # MTCNN 모델 가져오기
import torch
import numpy as np
import mmcv, cv2
from PIL import Image, ImageDraw
from IPython.display  import display
from torchvision import datasets, transforms
from tensorflow import keras
import joblib
# Dlib
import dlib, imutils
import numpy as np
from imutils import face_utils
import matplotlib.pyplot as plt
#HTML 영상 모니터링
from IPython.display import HTML
from base64 import b64encode

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # cuda GPU 를 사용하며, 없을 시 cpu 사용
print('Running on device: {}'.format(device))

mtcnn = MTCNN(keep_all=True, device=device) # mtcnn 모델 가져오기

v_path = "/content/drive/MyDrive/Colab Notebooks/test_park.mp4"

video = mmcv.VideoReader(v_path) # mmcv 비디오 읽어오기
frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video] # 프레임별로 cvt 시킨 후 저장

# 비디오 잘 가져왔는지 HTML 활용해서 확인
mp4 = open(v_path, 'rb').read()
data_url= "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)

# display.Video('video.mp4', width=640)

storage_face = [] # 탐지가 완료된 이미지 자체를 저장한 리스트
cutted_face_location = [] # 얼굴 부분만 잘라낸 좌표들을 저장하는 리스트
frame_list = []
# enumerate : 반복문 사용 시 몇 번째 반복문인지 확인할 떄 사용, 인덱스 번호와 원소를 tuple형태로 반환합니다.
for i, frame in enumerate(frames): # frame : 튜플형태로 사진 데이터 저장 / i : 인덱스 번호
    print('\r프레임: {}'.format(i + 1)) # 각각의 프레임 출력
    # 얼굴 탐지
    boxes, _ = mtcnn.detect(frame) # mtcnn 모델을 활용하여 한 프레임씩 탐지 후 박스에 결과 저장 
    # boxes : 2차원의 배열 (21, 4) # 행 : 감지된 사람의 얼굴 수 / 열 : 해당 정보의 차원 및 좌표
    # _ :  프레임 한 장에 탐지된 사람들의 얼굴 확률 값
    
    # 얼굴 그리기
    frame_draw = frame.copy() # 프레임 한 장 카피
    draw = ImageDraw.Draw(frame_draw) 
        
    
    try:
      # 감지된 사람 수가 0이라면 모델에 오류가 발생함 이를 해결하기 위한 알고리즘
      # 얼굴만 탐지하는 BOX
        for box in boxes:
            if box is not None :

                # 네모 그리기
                draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6) # tolist : list 형태로 대상의 차원에 맞춰서 반환, 얼굴 탐지가 된 좌표값
                #outline : 네모 색상 / width : 두께

                frame_list.append(frame_draw)
                cutted_face_location.append(box) # 검출된 얼굴 좌표


                # storage_face.append(frame_draw.resize((224, 224), Image.BILINEAR)) # 사이즈 축소 및 임시 저장소 리스트에 추가
                # BILINEAR : 이미지 리사이즈 시 이진선형을 사용하여 리사이즈 하겠다.
    except:
         print("except!!") # 가독성을 위한 띄어쓰기

print('탐색 끝!')

print(len(cutted_face_location))
frame_list[0]

# 랜드마크 모델 초기화
try:
  detector = dlib.get_frontal_face_detector()
# 모델 인스턴스화
  predictor = dlib.shape_predictor("/content/drive/MyDrive/Colab Notebooks/shape_predictor_68_face_landmarks.dat")
  print("read")
except:
  print("error")

type(cropedImage) # PIL image 타입 확인

# 탐지된 얼굴 좌표를 활용한 얼굴 이미지 추출
for i in range(len(frame_list)): # 프레임 개수
  x,y,w,h=cutted_face_location[i] # 얼굴 좌표 추출

  cropedImage=frame_list[i].crop((x,y,w,h)) # crop 을 활용한 얼귤좌표 자르기 (x,y,w,h) 튜플형태
  display(cropedImage) # 얼굴 이미지 출력

  # DLIB model
  numpy_image=np.array(cropedImage) # 이미지 넘파이 배열화

  image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR) # OpenCV 타입으로 변환
  imgae = cv2.resize(image,(480,480),interpolation = cv2.INTER_CUBIC) # 빈공간 큐빅 채우기
  image = imutils.resize(image, width=500) # imutils 로 다시 변환 및 리사이징
  # imutils 로 변환해야 랜드마크를 효율적으로 찍을 수 있게 된다. 

  color_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # imutils >> Opencv 다시 변환
  rectangles = detector(color_image, 1) # Detecting > rectangle

  for(i, rect) in enumerate(rectangles): # enumerate : numbering ~68
    shape = predictor(color_image, rect)
    shape = face_utils.shape_to_np(shape) # change to ndarray type for calculate
    
    (x,y,w,h) = face_utils.rect_to_bb(rect) # divide coordinates 
    cv2.rectangle(color_image, (x,y), (x+w, y+h), (0,255,255), 3) 
    ###
    # Flask를 통한 DB 에 사용자 vector 좌표 저장 
    ###
    cv2.putText(color_image, "Face #{}".format(i+1), (x-1, y-1),
                cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,0,255), 3) # 0.5 font-size
    # pointing Image
    for(x,y) in shape:
      cv2.circle(image,(x,y),3,(0,0,255),-1) # -1 : fully circle
      
      print(f"Count Faces : {i+1} (Left : {rect.left()} Top : {rect.top()} Right : {rect.right} Bottom : {rect.bottom()}")

  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(image)
  ########


  cropedImage.save(f"./cropedImages/number{i}.png")

dim = frame_list[0].size # (224, 224)
fourcc = cv2.VideoWriter_fourcc(*'FMP4')  
video_tracked = cv2.VideoWriter("/content/drive/MyDrive/Colab Notebooks/result.mp4", fourcc, 25.0, dim) 
# outputFile (str) – 저장할 파일 & 경로
# fourcc – Codec정보 >>  cv2.VideoWriter_fourcc()
# frame (float) – 초당 저장될 frame
# size (list) – 저장될 사이즈(ex; 640, 480)
for frame in frame_list:
   video_tracked.write(cv2.cvtColor(np.array(frame), 4)) # np 배열, cv2.COLOR_RGB2BGR = 4 색변환
video_tracked.release() # 열려있는 비디오 닫기 close == release

# 모델 저장
joblib.dump(mtcnn, './model_Park.pkl')

